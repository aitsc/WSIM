{fenge}
8844279977	Mining interesting knowledge from Web-log	Web-log contains a lot of information related with user activities on the Internet. How to mine user browsing interest patterns effectively is an important and challengeable research topic. On the analysis of the present algorithm's advantages and disadvantages, we propose a new concept: support-interest. Its key insight is that visitor will backtrack if they do not find the information where they expect. And the point from where they backtrack is the expected location for the page. We present User Access Matrix and the corresponding algorithm for discovering such expected locations that can handle page caching by the browser. Since the URL-URL matrix is a sparse matrix which can be represented by List of 3-tuples, we can mine user preferred sub-paths from the computation of this matrix. Accordingly, all the sub-paths are merged, and user preferred paths are formed. Experiments showed that it was accurate and scalable. It's suitable for website based application, such as to optimize website's topological structure or to design personalized services.
{fenge}
9444222585	Rough fuzzy integrals for information fusion and classification	This paper presents two extended fuzzy integrals under rough uncertainty, i.e. rough upper fuzzy and lower fuzzy integrals, and extended properties are also given. Furthermore, these two integrals are applied here in information fusion and classification processes for rough features, and the corresponding extended models are also proposed. These types of integrals generalize fuzzy integrals and enlarge their domains of applications in fusion and classification under rough uncertainty. Examples show that they fuse or classify objects with rough features with fairly good effects while the existed methods based on reals can not solve.
{fenge}
15744386127	Research on routing algorithm and self-configuration in content-based publish-subscribe system	Content-Based publish/subscribe systems have recently received an increasing attention. Efficient routing algorithms and self-configuration are two key issues in the area of large-scale content-based publish/subscribe systems. Although many routing algorithms have been proposed, none of them fully exploits multicast to enhance system performance and save network bandwidth. In addition, the vast majority of currently available publish-subscribe middleware has ignored this self-configuration problem. This paper first proposes a hierarchical system model with multicast clustering. Then a hybrid routing algorithm is presented, which can fully exploit multicast in order to reduce the used network bandwidth. Moreover, a multicast clustering replication protocol and a content-based multicast tree protocol are presented for coping with the node or link failures and rebuilding the event dispatcher trees. Experimental results reveal that the system has better routing efficiency and lower cost, and guarantees the self-configuration characteristic.
{fenge}
15744388479	Genetic algorithm for Web information query optimization	To efficiently retrieve relevant documents from the rapid proliferation of large information collections, a novel query optimization algorithm based on genetic algorithm is proposed. The population is organized into query niches to explore potential document space. The fitness function is computed based on query similarity with relevant documents and the number of query niche. Crossover operators based on term weight and similar terms are adopted for reproduction of new query population. Experimental results show that this algorithm has higher precision of document retrieval and faster computation speed compared with conventional query optimization algorithms.
{fenge}
15744395344	Mining of classification rule based on immune algorithm	To efficiently mine the classification rule from databases, a novel classification algorithm based on immune algorithm was proposed. The core of the immune classification algorithm is as follows. The rule antecedent is encoded as fixed-length chromosome. The fitness function is calculated according to minor misclassification ratio, simplicity and consistency of rules, and coverage ratio of training examples. A vaccination is accomplished by modifying genes on some bits in accordance with minimal fitness function which serves as prior knowledge. Immune selection is accomplished by testing whether a serious degeneration has happened in the evolutionary process and annealing selection. Meanwhile, a rule pruning procedure based on information gain was designed for improving the comprehensibility of classification rule mined. The algorithm has been compared with RISE and OCEC algorithms with five benchmark datasets from UCI data set repository. Experimental results show that the proposed algorithm not only has faster convergence speed, but also can achieve higher prediction accuracy with less number of rules.
{fenge}
15544373787	XML-based meta-retrieval of networked data sources	The XML-based meta-retrieval described in this paper is an integrated retrieval over multiple academic data sources on the premise that each data source has one web interface visible in the Internet environment. The goal of this model is to present an integrated retrieval interface in the Internet, to offer transparent accessing by formulating new queries suitable for heterogeneous data sources based on three adaptation rules, and to return the consolidated data results retrieved from multiple data sources. Technologies like eXtensible Markup Language(XML) and extensible Stylesheet Language Family(XSL) are used to facilitate the fulfillment of the goal. Experiment results prove that the model solve the heterogeneity of the data sources and provide transparent accessing for the end users. © 2004 IEEE.
{fenge}
15744364425	Web retrieval algorithm based on differential manifold	To further improve the retrieval performance of Web retrieval, an algorithm of Web retrieval based on differential manifold was proposed. The essential idea of the algorithm is as follows. Firstly, Web space was modeled as a manifold, and Web tangent space was simulated using a tangent plane of the manifold. Secondly, geodesic distance between points in the manifold was transformed into Euclidean distance between points in the 2-dimension plane. Finally, distance-based matching was performed only in the neighborhood of a query rather than in the whole document collection so as to the computational complexity of the matching process was reduced. Experimental results show that the algorithm surpasses over Google and IIT 35.24% and 40.93% in average precision respectively, and it is more appropriate for large and heterogeneous Web space than other algorithms.
{fenge}
15744364885	New algorithm for personalized topic distillation and hierarchical exploration	To interpret the procedure of hypertext induced topic search (HITS) based on a semantic relation model, the reason about the topic drift of HITS was found that Web pages are projected to a wrong latent semantic basis. A new algorithm for personalized topic distillation and hierarchical exploration (PTD-HE) was presented to improve the quality of topic distillation. Personalized root set and base set with query expansion was constructed using individual query logs to avoid the topic draft, and applying a hierarchical division algorithm based on min-max principle to explore relative topics of user query, and then using HITS to evaluate and return authority pages of relative topics to end-users. The experimental results on 14 queries show that PTDHE performs better than HITS in topic distillation quality and topic exploration ability. PTDHE reduces topic drift rate by 2% to 66% compared to that of HITS, and discovers several relative topics to queries that have multiple meanings.
{fenge}
1842421859	Secure authentication protocol for hierarchical trusted copyright management based on master/slavery license	To solve the problem of the security and efficiency which exists in traditional software protection approaches, a dynamic master/slavery-based secure authentication protocol supporting multilevel trusted copyright is proposed. A third part certificate authority (CA) is adopted for atomic authorization and forced revocation of a software license dynamically according to the software and hardware identity and their usage status, through which the security, consistency and re-distribution of the copyright can be enhanced. Thus the hierarchical trusted structure is suitable not only for single-client but for multi-client copyright management. During the interaction of the protocol, symmetric and public key cryptography algorithm are used for data encryption and digital signature. The result of the analysis has proved the feasibility, security and integrity of the proposed protocol, and as an extension of EULA, it provides a general scheme for software copyright protection.
{fenge}
19644368146	Study on processing method of missing values in personalized recommendation systems	To efficiently resolve the problem of missing data in cooperative filter algorithm (instead of simply using defaults), a new approach based on principal component analysis and fuzzy clustering was proposed. The essential idea was that an incomplete data set including missing values was partitioned into several fuzzy clusters by using local principle component with least variance, and through solving the general inverse matrix of the data to obtain the principle components of each sub-clusters, the missing values in clusters could be estimated based on local principal components utilizing a simple linear model. Experimental results show that this method is of low memory requirements and lower mean absolute error (MAE) value, and provides better recommendation quality compared with traditional collaborative filtering algorithms.
{fenge}
18744401341	Multi-engine fusion based on mixture model	In order to increase the performance of the combined retrieval system, a multi-engine fusion method based on a mixture model was presented. The method describes the relevant score distribution of the relevant and non-relevant documents using Gaussian density function and exponential density function respectively. Based on the algorithm of the mixture model the relevant scores are normalized, the scores of non-relevant documents are estimated and combined, which consider both the difference between relevant and non-relevant documents in the score distribution and the retrieval performances of the member search engine estimated by users. Experimental results show that the average search accuracy is improved by 37.8% compared with member engines, and also higher than three often used fusion methods of Sum-CombSUM, Sum-CombMNZ, and Standard-CombSUM.
{fenge}
18744404794	Rough set clustering algorithm based on entropy and information granularity	Aiming at most existing clustering algorithms that only handle the numeric data or categorical data rather than the mixed data, a clustering algorithm based on entropy and information granularity was proposed by using different granular partitions under the framework of the rough set theory. Using similarity relation and calculating the entropy at each data point, the data point with minimum entropy was selected as a clustering center. The numeric granules structure is formed by aggregating all data points in which the similarity with the chosen clustering center is larger than a threshold β. It does not need to regulate the entropy value at each data point in the clustering procedure, and saves the computation time. Moreover, the character granules structure is also formed by using indiscernibility relation in rough set. The cluster analysis with mixed attribute data is accomplished by iteratively modifying and agglomerating these two granules structures. The comparison of experimental results shows that the algorithm is effective and feasible. When β is 0.8, the maximum 0.96 of the clustering validity of the algorithm can be achieved, which is higher than others under same conditions.
{fenge}
18844410537	Research on reuse-based web services composition	Objective: To solve the three principal problems for reusing the software components, i. e., existence, discovery and availability principles, web services provided a new solution for reusing and assembling web software or components under distributed environment with a series of XML-based protocols. Methods: A 4 + 1 meta-model between atomic services, such as Sequence, Parallelism, Alternation, Substitute and Iteration, and a kind of service composition description language in composition process are investigated. Furthermore, an architecture-based service composition model with Ontology Engine is proposed, which is based on the software architecture life-cycle model and provides a mechanism for services composition with dynamic management and deployment. Results: The results of the prototype system development show that the service-composed application not only reused the web service components, but also reused the data resources located in distributed databases. Conclusion: The method of reused-based web services composition observably decreases the period and cost of development and greatly improves the programming efficiency.
{fenge}
19944365043	Real-time licensing management of secure multimedia content delivery	Media Commerce is now becoming a new trend which results from faster development of network bandwidth and high availability of multimedia technologies, how to protect media content from being used in a right-violated way is one of most important issues to take into account. In this paper, a novel and efficient authorization and authentication Digital Rights Management (DRM) schema is psoposed firstly for secure multimedia delivery, then based on the schema, a real-time digital signature algorithm built on Elliptic Curve Cryptography (ECC) is adopted for fast authentication and verification of licensing management, thus secure multimedia delivery via TCP/RTP can efficiently work with real-time transaction response and high Quality of Service (QoS). Performance evaluations manifest the proposed schema is secure, available for real-time media stream authentication and authorization without much effected of QoS. The proposed schema is not only available for Client/Server media service but can be easily extended to P2P and broadcasting network for trusted rights management.
{fenge}
20544454875	Generalized multi-layered granulations and approximations based on neighborhood systems under incomplete information systems	A generalized multi-layered granulation structure used by neighborhood systems is proposed. With granulated views, the concepts of approximations under incomplete information systems are studied, which are represented by covering of the universe. With respect to different levels of granulations, a pair of lower and upper approximations is defined and an approximation structure is investigated, which lead to a more general approximation structure. The generalized multi-layered granulation structure provides a basis of the proposed framework of granular computing. Using this framework, the interesting and useful results about information granulation and approximation reasoning can be obtained. This paper presents some useful explorations about the incomplete information systems from information views.
{fenge}
19944434171	Approach to construct a rough neural networks based on rough set	Aiming at the problem that clear physical meanings can't be given by nerve cells and weights of neural networks, a neural network model based on rough set was proposed, in which rules were extracted from given training data firstly by utilizing numerical analysis ability of rough set theory, and then neurons number of the hidden layer was determined in terms of these rules to obtain the original topology of the rough neural network. Meanwhile, the input to the model was mapped into the output subspace by using rules acquired from the rough set and the expectation output could be approximated. Thus, a neural network that provides with good understandability and rapid convergence could be constructed. Experiments show that the proposed approach can deal with problems of neural network topology architecture, sample size and quality which directly influence the generalization ability and accuracy of neural network. While greatly reducing training time, the prediction precision of the model can be achieved 96. 4% which is 3.6% higher than RBF (radial basis function) neural network model under the same conditions.
{fenge}
22844438010	Mining frequent pattern tree in Web data	To efficiently mine all frequent pattern trees from the semi-structured web data, the semi-structured data were modeled as labeled-ordered tree and an algorithm for mining all frequent pattern trees in an ordered data tree was proposed. This algorithm used rightmost path expansion technique, which started with pattern trees with only one node and nodes were added only to the rightmost path to generate new pattern trees. Furthermore, this algorithm maintained only the occurrences of the rightmost leaves to efficiently implement incremental computation of support. The theoretical analysis and experimental results show that this algorithm scales linearly in the total size of maximal tree pattern and works efficiently in practice.
{fenge}
21844433460	Rough reduction in fuzzy information systems	Based on rough set theories and fuzzy equivalence relations, a knowledge reduction method and importance degree of attributes under different granular partitions of the object space in fuzzy information systems (FISs) were presented. Two parameters for different level partitions (or similar degree between objects) α, β are used in these reductions, in which the positive region formula of the decision level set are adopted for relative reduction and importance degree of attributes, and the distributed reduction and the assignment reduction are obtained by using rough membership functions of the horizontal set. These reductions extend the attribute deduction methods in Pawlak information systems (PISs) and provide new tools for knowledge discovery and feature selection in FISs. Moreover, by using equivalence classes under different granules, the discernment attribute matrix and discernment formula of the distributed reduction and the assignment reduction were given, which overcome the inapplicability of classical methods in FISs. Demonstration results show that the attribute subsets having maximum degree of discernment and rule confidence with regard to all attributes can be produced by using these methods in different granular spaces.
{fenge}
2442640677	Logic and decision rules for fuzzy objective information systems	Based on fuzzy objective information system model, and combining with rough logic theory, a logic language, denoted as FRDL (fuzzy rough decision language), is proposed. The definitions, semantics, satisfiability and validity of FRDL and their related properties are given. The formulation and constructing ways of decision rules and reverse decision rules for fuzzy objective information systems are presented based on FRDL. The logic and reasoning rules described in FRDL provide decision support systems and inference machines, which are constructed by fuzzy objective information systems, with formal representation and reasoning method. The experimental results obtained from ophthalmological data, show that our formalization and reasoning methods are more reasonable.
{fenge}
24044490905	Retrieval algorithm combining multi-query data fusion with positive relevance feedback	In order to improve the performance of information retrieval system, a retrieval algorithm combining multi-query data fusion with positive relevance feedback is presented. The essential idea of the algorithm is as follows. The cosine similarity metric based on vector space model is used to measure the similarity between the query and documents; the retrieval results are fused by using multi-query data fusion technology; the new queries in the relevance feedback process are formed by combining the original query with the top M relevant documents from the results of the previous round retrieval, and then the new queries are used for the next round retrieval. The retrieval process is repeated until achieving satisfactory results. Experimental results show that in contrast to the algorithms that only using multi-query data technology and only using positive relevance feedback technology, the proposed algorithm increases the average precision by 42.6% and 23.17%, respectively.
{fenge}
24744449299	Survey of services composition technique research on web services	As a new value-added web services technique for reusing, web service composition has been gathering an increasing amount of attention lately. From the point of the development of distributed computing, this paper analyzes the characteristics and the life-cycle model of next generation of web services-based distributed computing. Furthermore, the conceptions, properties and goals about the composition of web services are reviewed, and the composition methods and classifications and composition logic and structures are analyzed. In addition, four kinds of web service composition strategies can be divided as framework-based static composition, workflow-based dynamic composition, semantic-based automatic composition and software architecture-based syntheses composition. Finally, the mainly research fields and trends of service composition technique and the vital problems and challenges existed in the process of composition are analyzed and reviewed.
{fenge}
24944453163	Automatic authentication technique based on supervised ART-2 and polynomial spline pyramid algorithm	This paper introduced a technique for authenticating the vehicle engines by comparing the images of the imprints of the identification number acquired when the vehicle was first registered and the ones acquired from the routine yearly vehicle inspection. The images are taken by rubbing a pencil over a piece of paper covered over the images and then are scanned into a computer. Due to the nature of the acquiring technique, the acquired images have lots of artifacts caused by the shape and the condition of the engine surface and unevenness of rubbing the pencils by hand. We used the polynomial spline pyramid algorithm to acquire a training set using ART-2, which is considered a tradeoff of stability-plasticity dilemma. The experiments show an accuracy rate close to 80%. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
24944461422	Content filtering of decentralized P2P search system based on heterogeneous neural networks ensemble	A Peer-to-Peer (P2P) based decentralized personalized information access system called PeerBridge for edge nodes of the Internet network is proposed to provide user-centered, content-sensitive, and high quality information search and discovery service from Web and P2P network timely. The general system architecture, user modeling and content filtering mechanism of Peer-Bridge are discussed in detail. Moreover in order to only find information which users are interested in, a new heterogeneous neural network ensemble (HNNE) classifier is presented for filtering irrelevant information, which combines several component neural networks to accomplish the same filtering task, and improves the generalization performance of a classification system. Performance evaluation in the experiments showed that PeerBridge is effective to search relevant information for individual users, and the filtering effect of the HNNE classifier is better than that of support vector machine, Naïve Bayes, and individual neural network. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
25144522596	A new method for retrieval based on relative entropy with smoothing	A new method for information retrieval based on relative entropy with different smoothing methods has been presented in this paper. The method builds a query language model and document language models respectively for the query and the documents. We rank the documents according to the relative entropies of the estimated document language models with respect to the estimated query language model. While estimating a document language, the efficiency of the smoothing method is considered, we select three popular and relatively efficient methods to smooth the document language model. The feedback documents are used to estimate a query model by the approach that we assume that the feedback documents are generated by a combined model in which one component is the feedback document language model and the other is the collection language model. Experimental results show that the method is effective and performs better than the basic language modeling approach. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
2542615197	Secure authentication protocol for trusted copyright management based on dynamic license	Trusted software copyright protection is one of the most important issues in digital rights management. However, most of the current solutions could not meet the demand of End User License Agreement (EULA) in security and efficiency. In this paper, a new and secure authentication protocol for trusted copyright protection based on dynamic license is proposed to solve the above problem. A third part Certificate Authority (CA) is adopted for an atomic authorization and a forced revocation of software license dynamically according to the software and hardware identity and their usage status. Thus under the control of the dynamic license, the copyright is protected safely and the software entity can be transferred freely without copyright damage and resource leakage. Considering the integrity and security of the dynamic license, symmetric and public key cryptography algorithms are used for data encryption and digital signature respectively, while random verification of coding signature is adopted to resist possible attack and runtime crack. Analysis manifests that the proposed protocol is feasible and secure with a high integrity. It can meet the demand of EULA and provide a new and reliable approach for software copyright management.
{fenge}
26944459234	Retrieval based on language model with relative entropy and feedback	A new method for information retrieval which is on the basis of language model with relative entropy and feedback is presented in this paper. The method builds a query language model and document language models respectively for the query and the documents. We rank the documents according to the relative entropies of the estimated document language models with respect to the estimated query language model. The feedback documents are used to estimate a query model by the approach that we assume that the feedback documents are generated by a combined model in which one component is the feedback document language model and the other is the collection language model. Experimental results show that the method is effective for feedback documents and performs better than the basic language modeling approach. The results also indicate that the performance of the method is sensitive to both the smoothing parameters and the interpolation coefficients used to estimate the values of the language models. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
26944476262	Ontology-based searching over multiple networked data sources	In this paper we propose an ontology model to describe the domain of searching over multiple data sources (SND). The ultimate goal of the model is to bridge the heterogeneity and to solve the distribution of the multiple networked data sources. Based on the model a query reconstruction algorithm is proposed to translate the initial query into different local queries suitable for individual data sources. And the result processing module defines grammar rules to extract the useful data returned by the individual data source, consolidates the retrieved data into one unified result, and presents the results in the web browser. In the ontology model two constraint rules are defined to evaluate the performance of SND and experiment results show that the federated retrieval offers a reliable and efficient reproduction with the retrieved results from the independent data sources. Moreover, the figures indicate the time comparisons among multiple data sources and give the contribution of each data source by the returned searching results based on the same query set. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
26944478551	Extracting classification rules with support rough neural networks	Classification is an important theme in data mining. Rough sets and neural networks are two technologies frequently applied to data mining tasks. Integrating the advantages of two approaches, this paper presents a hybrid system to extract efficiently classification rules from a decision table. The neural network system and rough set theory are completely integrated to into a hybrid system and use cooperatively for classification support. Through rough set approach a decision table is first reduced by removing redundant attributes without any classification information loss. Then a rough neural network is trained to extract the rules set form the reduced decision table. Finally, classification rules are generated from the reduced decision table by rough neural network. In addition, a new algorithm of finding a reduct and a new algorithm of rule generation from a decision table are also proposed. The effectiveness of our approach is verified by the experiments comparing with traditional rough set approach. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
27744447177	Multidimensional latent semantic query using Hilbert space filling curve in peer-to-peer network	A new method called multidimensional latent semantic query (MLSQ) was presented in order to provide efficient multi-keyword query and search mechanism in structured P2P (peer-to-peer) systems. With Hilbert space filling curve (HSFC) and latent semantic index (LSI), MLSQ maps similar data objects in high dimension semantic space into an adjacent range in one dimensional value space, and considers every data object sequence number in one dimensional space as a key. MLSQ distributes the keys to adjacent nodes of structured P2P systems. By embedding the process of searching data points of HSFC into the nodes of structured P2P networks and using the data search mechanism of networks, MLSQ can facilitate to search the data objects in accordance with query requirements. The theoretical analysis and experimental results show that MLSQ can improve the query accuracy and reduce the number of communication messages, so MLSQ is better than the existing multidimensional query based on keywords matching in structured P2P networks.
{fenge}
28344447796	Architecture-based web service composition framework and strategy	Web services provided a new solution for reusing and assembling web sofrware or components under the distributed service-oriented architecture and across-platform environment with a series of XML-based protocols. An architecture-based service composition model based on the architecture life-cycle is proposed, which provide a mechanism for dynamic service management and deployment. It also guarantees the version control of service components and a manageable evolvement process of application system. In addition, a kind of relationship-oriented service composition description language, i.e., RSCDL, for description the composition process is investigated, which can guarantee that the complex services can be composed of the reused atomic services by different structural types and can provide a value-added service to different users. © 2005 IEEE.
{fenge}
28444454957	A dynamic fault tolerant algorithm for improvising performance of multimedia services	Multimedia Services has drawn much attention from both industrial and academic researchers due to the emerging consumer market, how to provide High-Availability service is one of most important issues to take into account. In this paper, a dynamic fault tolerant algorithm is presented for highly available distributed multimedia service, then by introducing SLB(server load balancing) into fault tolerance and switching servers in different ways according to their functions, the proposed schema can preserve reliability and real-time of the system . The analysis and experiments indicate that resuming server's faulty by this method is smooth and transparent to the client. The proposed algorithm is effectively improving the reliability of the multimedia service.
{fenge}
30044442481	ALBC4WS: a dynamic serivce composition framework based on the software architecture lifecycle	As a newly distributed computing paradigm under the loose coupling Internet environment, Web service provides a new technology for B2B dynamic e-business across different organizations. Some conceptions about Web service composition are defined and some properties are analyzed from the architecture perspective. A Web service composition model based on software architecture life cycle, i.e. ALBC4WS model, is proposed, which integrates the merits of existing composition strategies and is built on the basis of service publish management algorithm and service automatic inquiry-and-resume management for composition algorithm. Therefore, this model provides a dynamic and manageable underlying framework for service composition. The application results in the process of the development of OPEN-WEB prototype system show that the ALBC4WS model effectively increases the composition capability of service component and provides the robustness and self-adaptability of service composition-based application, while it also provides a dynamic management mechanism for service automatic composition.
{fenge}
33644660515	Incremental clustering algorithm via cross-entropy	A new incremental clustering method is presented, which partitions dynamic data sets by mapping data points in high dimension space into low dimension space based on (fuzzy) cross-entropy (CE). This algorithm is divided into two parts: initial clustering process and incremental clustering process. The former calculates fuzzy cross-entropy or cross-entropy of one point relative to others and a hierachical method based on cross-entropy is used for clustering static data sets. Moreover, it has the lower time complexity. The latter assigns new points to the suitable cluster by calculating membership of data point to existed centers based on the cross-entropy measure. Experimental comparisons show the proposed method has lower time complexity than common methods in the large-scale data situations or dynamic work environments.
{fenge}
33645154900	Design of information extracting modularization for context-free languages	Lex and Yacc are used to analyze the Lexis and grammar of the languages defined by context-free grammars, and an independent information extracting module is built to extract the information of source files for white-box testing. The whole grammar system for a tested language accorded with the criterion of Yacc is built, and the system is logically divided into two primary parts, class grammar and function grammar. Sentence structure and sentence linked list structure express the function definition using block and level when they extract function information. The structure information is put into the database to be used for other modules, thus the modules are completely independent. In this way, it is only needed to design different information extraction modules, various context-free grammar languages for white-box testing can be realized.
{fenge}
33646145725	Study on the recognition and utilization of building block in the iterations of genetic algorithm	A genetic algorithm (GA) based on building block recognition was proposed, in which building block candidates were recognized in evolving process to speed up the search so as to avoid the blindness of GA random searching. The typical symmetric TSP (traveling salesman problem) solving process was used to test various recognition methods, and then the test results were used to improve the traditional GA, including improving the recognition rate of building blocks and mutation and crossover operators based on building block. Compared with the computational result of traditional GA, it shows that the searching efficiency of GA can be improved remarkably and the fluctuation of random searching can be reduced by recognizing building block.
{fenge}
33646146715	Analysis of overloaded model of concurrent multimedia service systems	An abstract model of multimedia servers was built under the condition of overload according to the queueing theory. Thus, the VOD (video on demand) system was reduced to an M/M/1 queue and many closed-queues with limited resources N. By analyzing the model, it shows that the available QoS (quality of service) and service with high reliability are related to the number of concurrent multimedia servers N and the number of service instances n provided by a multimedia server in the system. The higher QoS can be obtained by adjusting the ratio of N to n, with which the different high reliability services are achieved. Simulation tests indicate that the coincidence between the theoretical and experimental results of the model is up to 95% under the overload condition. This model can guide the design of multimedia systems effectively.
{fenge}
33646159959	Fuzzy integral method to merge search engine results	A fuzzy integral algorithm of the distributed information retrieval is derived and given to merge the distributed information retrieval result set. The result set is merged and the evaluation is ranked by using the monotonicity of fuzzy integral and calculating the fuzzy measure values. Four different search engines are tested in the practical web environment, and the results show that the algorithm can balance chorus effect and dark horse effect better after fuzzy integral is carried out, and the information merger can be obtained well. Under same condition, on the top of 100 ranked documents, the number of documents obtained by the proposed integral algorithm is more than Borda Count algorithm 3-4 items, and ComMIN (ComMAX) algorithm 7-8 items, respectively.
{fenge}
33646823976	A new integrated personalized recommendation algorithm	Traditional information retrieval technologies can satisfy users' needs to some extent. But they cannot satisfy any query from different backgrounds, with different intentions and at different time because of their all-purpose characteristics. An integrated searching algorithm by combining filtering with collaborative technologies is presented in this paper. The user model is represented as the probability distribution over the domain classification model. A method of computing similarity and a method of revising user model are provided. Compared with the vector space model, the probability model is more effective on describing users' interests. Furthermore, collaborative-based technologies are used, and as a result the scalability of the new algorithm is highly enhanced. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
33646853511	A fuzzy integral method to merge search engine results on web	Distributed information retrieval searches information among many disjoint databases or search engine results and merge of retrieved results into a single result list that a person can browse easily. How to merge the results returned by selected search engine is an important subproblem of the distributed information retrieval task, because every search engine has its own calculation or definition about relevance of documents and has different overlap range. This article presents a fuzzy integral algorithm to solve the merging results problem. We have also a procedure for adjusting fuzzy measure parameters by training. Compared to the method of relevance scores fusion and Borda count fusion, our approach has the excellent ability to balance between chore effects and dark horse effects. The experiments on web show that our approach gets better ranked results (more useful documents on top ranked). © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
33744473358	Research of the Chinese meta-search engine model based on intelligent agent	This paper discussed the disadvantages of the current search engines and built a meta-search engine. The system constructed the vector set, which composed of the vectors of each domain, by using the materials hading been classified manually. When the user visited the Internet, the historical pages were recorded and downloaded. The system extracted the information from the pages and constructed the vector of every page. The distance between the page vector and the domain vector was calculated. The more the distance the more the relativity. The personalized model of the given user was built by the relativity. Due to the limit of the dictionary, the method of the word segmentation called "a domain independent dictionary free lexical acquisition model for Chinese document" was used. In this paper, some key technologies related to the system were discussed, such as extracting the class keywords, classifying methods, and the algorithm of the page value calculating. In the last paragraph, the future works, such as the storing of semi-structure information, and the calculating of the web page's value were mentioned. © 2005 IEEE.
{fenge}
33745282434	Experimental study on friction factor and numerical simulation on flow and heat transfer in an alternating elliptical axis tube	This paper focuses on the experimental study on friction factor and the numerical simulation on the periodic fully developed fluid flow and heat transfer in an alternating elliptical axis tube (AEAT). The experimental results show that in the laminar flow regime fRe = 84.7, and the transition from laminar to turbulent flow occurs at an earlier Reynolds number about 1000. The predicted cycle average Nusselt numbers from the standard k-ε model and RNG k-ε model are quite close to each other, which are appreciably higher than that of elliptic tube and round tube. Heat transfer performance comparisons are made under identical pumping power constraint, showing the obvious superiority of AEAT over a round tubes. In addition, the complicated multi-longitudinal vortex structure of the flow is detected in detail from the numerical simulation results, which improves the synergy between velocity field and temperature gradient in a large extent, hence, greatly enhancing the convective heat transfer. © 2006 Elsevier Ltd. All rights reserved.
{fenge}
33744949611	High-availability VoD system with dynamic fault tolerant	In this paper, a dynamic fault tolerant algorithm is presented for highly available distributed video on demand (VoD) systems. According to the performance requirements of the distributed applications, the load balancing is introduced into fault tolerance. Different server switching methods in different ways according to their functions are presented, which preserve the reliability and real-time of the system. System consistency is guaranteed after server switching, which makes the clients unaware of change. The switching between the faulty server and the new server is smooth and it is transparent to the client. Feasibility of this algorithm is analyzed. Experiments indicate that the algorithm effectively improves the reliability of the VoD systems.
{fenge}
33646748357	Unsupervised rough clustering method based on Gaussian mixture model	Aiming at resolving randomness and complexity of data statistical distribution, the whole data probability density function is described by Gaussian mixture model in the sight of statistical clustering. A rough clustering analysis method based on Gaussian mixture model is proposed. Firstly, the initial parameters of EM obtained by indiscernibility relation and logic rules generated with rough set theory. Secondly, the maximum likelihood parameters of each component probability density distribution can be estimated by EM iterative computation. Finally, the classification is determined through density distribution probability value. Experimental results show that the new method is effective. Compared with conventional k-means clustering algorithm, it has higher clustering precision and the clustering results described by the rule sets are interpretable and rational.
{fenge}
33745876529	Training RBF neural network with hybrid particle swarm optimization	The particle swarm optimization (PSO) has been used to train neural networks. But the particles collapse so quickly that it exits a potentially dangerous stagnation characteristic, which would make it impossible to arrive at the global optimum. In this paper, a hybrid PSO with simulated annealing and Chaos search technique (HPSO) is adopted to solve this problem. The HPSO is proposed to train radial basis function (RBF) neural network. Benchmark function optimization and dataset classification problems (Iris, Glass, Wine and New-thyroid) experimental results demonstrate the effectiveness and efficiency of the proposed algorithm. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
33746482064	Automatic authentication based on inexact graph matching	This paper introduced a technique for authenticating the vehicle engines. It has been compared by the images of the imprints of the identification number acquired when the vehicle was first registered and the ones acquired from the routine yearly vehicle inspection. Hough Transform algorithm was used to extract line feature from the images of the imprints, which characterized the identification number. Furthermore, line feature was combined with tolerance relation on a set, an algorithm based on inexact graph matching for authentication was presented. This had dealt with incomplete data owing to lots of artificial. The experiments showed an accuracy rate close to 83%.
{fenge}
33745899329	Optimizing personalized retrieval system based on web ranking	This paper drew up a personalized recommander system model combined the text categorization with the pagerank. The document or the page was considered in two sides: the content of the document and the domain it belonged to. The features were extracted in order to form the feature vector, which would be used in computing the difference between the documents or keywords with the user's interests and the given domain. It set up the structure of four block levels in information management of a website. The link information was downloaded in the domain block level, which is the top level of the structure. In the host block level, the links were divided into two parts, the inter-link and the iutra-link. All links were setup with different weights. The stationary eigenvector of the link matrix was calculated. The final order of documents was determined by the vector distance and the eigenvector of the link matrix. © Springer-Vorlag Berlin Heidelberg 2006.
{fenge}
33746664224	Attribute reduction based expected outputs generation for statistical software testing	A lot of test cases need to be executed in statistical software testing. A test case consists of a set of inputs and a list of expected outputs. To automatically generate the expected outputs for a lot of test cases is rather difficult. An attribute reduction based approach is proposed in this paper to automatically generate the expected outputs. In this approach the input and output variables of a software are expressed as conditional attributes and decision attributes respectively. The relationship between input and output variables are then obtained by attribute reduction. Thus, the expected outputs for a lot of test sets are automatically generated via the relationship. Finally, a case study and the comparison results are presented, which show that the method is effective. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
33748681800	Coverage criteria for GUI testing based on directed graph for window navigation	To evaluate the adequacy of a test suite for testing a program with graphical user interfaces (GUI), new coverage criteria based on directed graph for window navigation was proposed. The vertexes in the graph represent windows of GUI, and the directed edges represent user inputs on windows. The graph models the interaction between user and GUI. Top window coverage criterion, user input coverage criterion and length n user input sequence coverage criterion were proposed based on the graph. Algorithms were presented to compute the coverage rate on every coverage criterion. Experimental results show that the test suites satisfying the proposed criteria can reach a statement coverage rate as high as 90%, and a GUI fault detection rate as high as 79%.
{fenge}
33747817446	Improved adaptive GA application on automatic test data generation	The current research of software test data automatic generation is not sufficient. To express the structure of class chart perfectly, a novel storage strategy of chart, bidirectional adjacent list, is presented. An improved adaptive genetic algorithm (iAGA) using instrumentation technique based on information extraction from source file and a novel construct fitness function using tree structure are developed for automatic software test data generation. The performance comparison was made between SGA, AGA and iAGA. The experimental results show that the iAGA may significantly reduce the seeking time and generation and the iAGA is effective and promising. Average weight, Top-to-down, and Down-to-top assign methods are also compared, it shows that the average weight assign method is more excellent than others under tree structure fitness calculating.
{fenge}
33747041091	Hybrid particle swarm optimization strategy with adaptive mutation and its applications	A novel hybrid particle swarm optimization (PSO) based on adaptive population mutation and individual annealing operation was developed. The simulated annealing (SA) operation was introduced into the PSO. Regarding PSO as the principal part of the hybrid strategy, initial colony was randomly generated firstly, and then new individuals were searched. Meanwhile, the adaptive mutation and annealing operation were used to adjust and optimize the population. Compared with SA and basic PSO, the hybrid PSO (HPSO) keeps the simple and convenient character of the standard PSO, and also carries on mutation adaptively. Experimental results for some complex function optimization and several TSP combination optimization problems show that the HPSO improves the global convergence ability and efficiently prevents the algorithm from the local optimization and early mature.
{fenge}
33750215455	ANN model based on Pi calculus	Artificial neural network (ANN) is an important method of artificial intelligence, but ANN lacks an all-purpose mathematical model, which can simulate all kinds of neuron architecture. At present, formal method is an effective method for modeling and verifying system. Therefore it is an important research direction to describe and verify ANN by formal method. The Pi calculus is a kind of mobile process algebra that can be used to model concurrent and dynamic system. Therefore, both Pi calculus and ANN are on the basis of concurrent computation. This paper firstly introduces a method to use Pi calculus for modeling ANN and illustrating equivalence between them. Then on the basis of this, an artificial neural network classifier based on Pi calculus, which takes the same basic principles of concurrent computation as neural networks, is discussed in this paper. Finally, its superiority is also discussed.
{fenge}
33749591881	A model-driven approach for business constraints discovery	Business constraints restrict the enterprise structure, govern the business behavior and contribute to the business goals. Due to stuff turnover and lack of documentation the knowledge of what constraints exist and how they interact are often poorly defined. This can seriously hinder an organization to understand how the business behaves and operates and to seize new opportunities or improve the business performance. A model-driven discovery approach is proposed to help the user to uncover the business constraints from the enterprise documents. This approach proposes a meta-model to describe the ontological constructs of the business constraints enforcing environment, defines a set of formal templates to document the business constraints to facilitate communication, sharing and reuse, gives a verification algorithm to ensure the completeness and validness of the discovered business constraints, defines the interactions among business constraints and finally proposes an elicitation procedure to efficiently guide the business users to discover the business constraints. Moreover, an example is presented to demonstrate the discovery approach of the business constraints and their usage for decision-making. © 2006 Asian Network for Scientific Information.
{fenge}
33749605660	Important usage paths selection for GUI software testing	From the user's point of view, the reliability of software depends greatly on the manner in which the software is used. As a result, it is necessary to test the software according to some model that highlights critical usage. Windows Navigation Networks (WNN) were proposed to model the usage of GUI software. Vertexes in the model represent windows and arcs represent transitions between windows. Each transition has probability of occurrence. Algorithm was proposed to obtain the transition probability from software usage log automatically. Important paths can be selected based on the WNN. Existing testing technologies then be used to test the important paths. WNN can describe the usage of GUI software from users' view and reduce the complexity of modeling GUI software. Important usage paths of GUI software can be got from WNN. It can focus the testing to reveal more important faults. © 2006 Asian Network for Scientific Information.
{fenge}
33749628864	Adaptive SAGA based on mutative scale chaos optimization strategy	A hybrid adaptive SAGA based on mutative scale chaos optimization strategy (CASAGA) is proposed to solve the slow convergence, incident getting into local optimum characteristics of the Standard Genetic Algorithm (SGA). The algorithm combined the parallel searching structure of Genetic Algorithm (GA) with the probabilistic jumping property of Simulated Annealing (SA), also used adaptive crossover and mutation operators. The mutative scale Chaos optimization strategy was used to accelerate the optimum seeking. Compared with SGA and MSCGA on some complex function optimization and several TSP combination optimization problems, the CASAGA improved the global convergence ability and enhanced the capability of breaking away from local optimal solution. © 2006 Asian Network for Scientific Information.
{fenge}
33749635125	A constraint-centered two-layered meta-model for enterprise modeling and analysis	Enterprise model is a representation of the knowledge an organization has about itself and of what it would like this knowledge to be. The changing business environment makes a big challenge for the continuous improvement from as-is to to-be of the enterprise. Business constraints are defined as relationships maintained or enforced in the business structure and process to form the most volatile part of the business requirements. The proposed meta-model for enterprise modeling introduces the business constraints as the key element to make the model responsive to the business changes. It consists of two layers including conceptual layer and specification layer where the former provides a foundation for better communication and the latter provides business constraint language and constraint flow language to accommodate the changes and decrease the model maintaining effort. The meta-model forms a precise business as-is description based on which an Analytic Hierarchy Process based what-if analysis mechanism is proposed to analyze the business alternatives to give recommendations for the to-be design. Examples are presented to demonstrate the validness of the metamodel and its what-if analysis mechanism. © 2006 Asian Network for Scientific Information.
{fenge}
33749856992	New PR-combining TFIDF with pagerank	TFIDF was widely used in IR system based on the vector space model (VSM). Pagerank was used in systems based on hyperlink structure such as Google. It was necessary to develop a technique combining the advantages of two systems. In this paper, we drew up a framework by using the content of web pages and the out-link information synchronously. We set up a matrix M, which composed of out-link information and the relevant value of web pages with the given query. The relevant value was denoted by TFIDF. We got the NewPR (New Pagerank) by solving the equation with the coefficient M. Experimental results showed that more pages, which were more important both in content and hyper-link sides, were selected. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
33750708220	Neural networks based automated test oracle for software testing	A lot of test cases must be executed in statistical software testing to simulate the usage of software. Therefore automated oracle is needed to automatically generate the expected outputs for these test cases and compare the actual outputs with them. An attempt has been made in this paper to use neural networks as automated test oracle. The oracle generates the approximate output that is close to expected output. The actual output from the application under test is then compared with the approximate output to validate the correctness. By the method, oracle can be automated. It is of potential application in software testing. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
33751417163	Statistical software test based on Markov chain path usage model	A Markov chain model based on path usage model is developed to analyze the inner code structure in statistical software test. The statistical theory is used on software structure test to obtain the reliability of the software structure theoretically. A Markov chain with finite state, scattered time sequence is put forward as the usage model and test model of the program. The Kullback discriminant is used as the convergence judgment criteria of the test-chain to usage-chain, and the inevitability of the convergence is testified theoretically. The primary experiments and theorized analysis prove the method is approving and promising.
{fenge}
33845355235	Component-based software development process model	To the component-based software engineering(CBSE), this paper proposes an enterprise software building model. Unlike waterfall model, this model unfolds each phase around reusing and reorganizing of existing architecture/component; and each activities in every phase is a self-contained unit, which incrementally reiterates steps to find, adopt, evaluate, and / or create new architecture / component. It is not a one-way process, the feedback from each phase and activity will propagate up and down the process streams to start new iteration. This paper also discusses the software industry organization structure, and shares opinions on the standardized production and management of software components. The application results show the average delivering time can be reduced 31%, and the cost can be reduced 28%.
{fenge}
33847122098	Adaptive SAGA based on mutative scale chaos optimization strategy	A hybrid adaptive SAGA based on mutative scale chaos optimization strategy (CASAGA) is proposed to solve the slow convergence, incident getting into local optimum characteristics of the standard Genetic Algorithm (SGA). The algorithm combined the parallel searching structure of Genetic Algorithm (GA) with the probabilistic jumping property of Simulated Annealing (SA), also used adaptive crossover and mutation operators. The mutative scale Chaos optimization strategy was used to accelerate the optimum seeking. By comparing the CASAGA with SGA and MSCGA on effectiveness, the CASAGA has more strong searching ability than other two, it can abandon the local optimal solution and find the global one more quickly. © 2005 IEEE.
{fenge}
33847163224	A kind of SAaGA hybrid meta-heuristic algorithm for the automatic test data generation	Test data generation is very labor-intensive and expensive in software testing. The automation of test process can achieve significant reductions in the cost of software development. Combining the parallel search ability of the adaptive genetic algorithm (aGA) with the controllable jumping property of simulated annealing (SA), a kind of effective hybrid meta-heuristic algorithm (SAaGA) with the operators and parameters well designed is proposed for automatic test data generation in this paper. The experiments have shown that the SAaGA hybrid meta-heuristic algorithm may significantly reduce the cost and improve the percentage of code coverage compared to the genetic algorithm and simulated annealing. © 2005 IEEE.
{fenge}
33847172307	An improved genetic algorithm for flow shop sequencing	Flow shop sequencing is one of the most well-known production scheduling problems and a typical NP-hard combinatorial optimization problem with strong engineering background. To efficiently deal with flow shop sequencing problems, an improved Genetic Algorithm using novel adaptive genetic operators is proposed. Researches are made in aspects such as problem modeling, encoding, decoding, crossover and mutation of Genetic Algorithms and so on. The proposed algorithm has been tested on scheduling problem benchmarks. Experimental results show that improved Genetic Algorithm is quite flexible with satisfactory results, and require fewer running time than pure Genetic Algorithms and Simulated Annealing. © 2005 IEEE.
{fenge}
34247117852	A robust algorithm for subspace clustering of high-dimensional data	Subspace clustering has been studied extensively and widely since traditional algorithms are ineffective in high-dimensional data spaces. Firstly, they were sensitive to noises, which are inevitable in high-dimensional data spaces; secondly, they were too severely dependent on some distance metrics, which cannot act as virtual indicators as in high-dimensional data spaces; thirdly, they often use a global threshold, but different groups of features behave differently in various dimensional subspaces. Accordingly, traditional clustering algorithms are not suitable in high-dimensional spaces. On the analysis of the advantages and disadvantages inherent to the traditional clustering algorithm, we propose a robust algorithm JPA (Joining-Pruning Algorithm). Our algorithm is based on an efficient two-phase architecture. The experiments show that our algorithm achieves a significant gain of runtime and quality in comparison to nowadays subspace clustering algorithms. © 2007 Asian Network for Scientific Information.
{fenge}
34247133993	Study on mutual information based clustering algorithm	Traditional clustering algorithms are designed for isolated datasets. But in most cases, relationships among different datasets are always existed. So we must consider the actual circumstances from the cooperative aspects. A new collaborative model is proposed and based on this model a new cooperative clustering algorithm is presented. In theorem, the algorithm is proved to converge to the local minimum. Finally, experimental results demonstrate that the clustering structures obtained by new algorithm are different from those of conventional algorithms for the consideration of collaboration and the performances of these collaborative clustering algorithms can be much better than those traditional separated algorithms under the cooperating circumstances. © 2007 Asian Network for Scientific Information.
{fenge}
34047208768	Component constraint detection based on proposition logic	Aiming at the problem that there exists very complicated and a large amount of component constraints, an algorithm of component constraint detection based on proposition logic was proposed, in which the proposition in daily diction was transformed into the formal proposition of mathematical logic via the process of proposition symbolization, i.e. five connective words. The proposed method utilizes the concept of constrained truth table to describe the component constraints, and the software architecture, dynamic behavior and transformation among components are revealed transparently and exactly. The truth table is incorporated by inclusion, negation and insertion rules to solve the problem of redundancy and conflict existing in detecting the component constraints. Moreover, a minimal set of rules of the expected behavior is obtained to ensure the consistency of component constraints. Compared to the existing rule pattern table approach, the uncertainty in manual detection is overcome and the temporal performance of detection is improved. The misjudgment of conflict is decreased by 10% and the average processing time is lowered by about 33%.
{fenge}
34047230968	Seismic reservoir oil-gas prediction study based on rough set and RBF network	In predicting oil-gas reserves in the course of seismic prospecting, a direct employment of neural network will take up enormous storage, lead to prolonged computing time and complicate the structure thanks to the numerous input information dimensions of seismic attributes. To solve the problem, taking into account the characteristics of oil-gas seismic attributes, the paper proposes a prospecting approach based on rough set attribute reduction and radial basis function neural network (RBFNN). That is to minimize the seismic attributes adopting a rough set reduction algorithm, which will simplify the input structure of neural network as well as cutting down on the time-needed for study and training. The adoption of RBFNN as a prediction system can overcome weaknesses typical of tradition BP network and enable quicker and more stable study and training of the calculation simulated tests and experiments reveal that the network constructed by means of sample attribute reduction is able to meet the precision requirement of prediction and to save costs and enhance processing speed, which has been proved effective in oil-gas prediction. ©2006 IEEE.
{fenge}
34047234164	Complete algorithm of increment for attribute reduction based on discernibility matrix	Aiming at the problem that complete algorithm of attribute reduction based on discernibility matrix can not find approximately minimal reduction, an improved attribute reduction method is proposed based on the original algorithm, and the attribute importance defined from the viewpoint of information theory is regarded as heuristic information. The discriminated set is operated by constructing an operator of conditional information entropy, and the excluding order of candidate attributes is calculated as the algorithm is iterated. Meanwhile, the breadth-first search strategy is utilized to make minimal reduction set contain the most important attribute. Thus, the method can resolve the problem that complete algorithm is of low reduction rate. On the basis of the analysis of the relation between the objects increase and discernibility matrix, a theorem of the increment reduction is proved, and then a complete algorithm for increment reduction (CAIR) is presented. When new data are added into the decision table, the discernibility set can be constructed incrementally. The experimental results show that the computation time of the discernibility set is significantly reduced by CAIR, the reduction rate is 20.3% higher than that of incomplete algorithm, and the execution efficiency is enhanced by 13.2 times compared to the complete algorithm under same conditions.
{fenge}
34047241166	Neural networks based test cases selection strategy for GUI testing	The purpose of Graphical User Interfaces (GUI) testing is to diagnose and expose faults in planning time. It is difficult because the input space of GUI is extremely large due to different permutations of inputs and events. To test GUI needs to run a lot of test cases. Neural networks (NN) were explored to reduce test cases to expose new faults. The main idea is as follows. Firstly, NN was trained by subset of test cases that had executed and their test results. Trained NN could recognize fault patterns that had been exposed. Secondly, from the test suite that hadn't been executed, trained NN was used to select test cases that don't belong to the fault patterns. The test cases selected were more likely to expose new faults in GUI. By the method new faults could be exposed by executing fewer test cases. The experimental results show that the strategy is effective. © 2006 IEEE.
{fenge}
34248166703	Automated oracle based on multi-weighted neural networks for GUI testing	Graphical User Interfaces (GUI) software has characteristics different from traditional software. The oracle for GUI software testing must validate the correctness of the GUI. An automated oracle based on multi-weighted Neural Networks (NN) is proposed in this paper to validate the GUI from users' viewpoint. In this approach the multi-weighted NN is used to learn the topological information in the feature space for the expected images of the graphical interface. The topological information is then used to verify the correctness of the GUI. By this method, trivial difference in the graphical interfaces can be ignored and GUI be automatically tested in the manner of human being. Experimental results show the method is of potential application in automated GUI testing. © 2007 Asian Network for Scientific Information.
{fenge}
34249329947	Efficient genetic reduction algorithm for rough sets	The concept and algorithm of the simplified discernment function are provided. The function not only has the same decision ability as the decision table, but also eliminates all the reduplicated and redundant terms included in the original discernment function educed by the decision table. To reduce the search space of the fitness function and improve the computing efficiency, an efficient genetic reduction algorithm is provided. By regarding the chromosome coverage of the simplified discernment function and the number of 1 in the chromosome as the parameters of the fitness function, it can be ensured that the algorithm will converge at the minimum reduction and improve the searching efficiency. It is proved theoretically that the attribute reduction calculated by the algorithm is optimal and the algorithm complexity is O(|f'||C||U|
{fenge}
34447133442	New algorithm for quick computing core	In order to improve the speed and efficiency of reduction algorithm of core influence attributes, a new core computation algorithm based on positive region is provided. The positive region based on radix sorting is used to get the positive region condition attributes set of decision attributes and the condition attribute set which excludes one of the condition attributes of decision attribute positive region. Then the difference between the two radices of positive regions is calculated to judge whether the condition attribute is a core attribute, thereby all the condition attributes are judged and the required core is quickly acquired. The time complexity of the proposed algorithm is O(|C||U|). Experimental results show that the core computation time increases linearly with the increasing number of entries, and the computation time is only 0.6% of the contrastive algorithm when the entries is maximum. Meanwhile, it is shown that the algorithm is well suitable for various kinds of data sets.
{fenge}
4043050091	Focused crawling method with online-incremental adaptive learning	Almost current focused crawling systems need volume of trained data samples and cannot learn persistently. Based on the principle of the reinforcement learning, the Web crawling is viewed as a process to perform sequential actions. Combining with the improved fast Q-learning and semi-supervised Bayesian classifier, a novel focused crawling method being able to make online-incremental adaptive learning was presented. Using the characteristic texts extracted from the retrieved pages, the topic-relevance of the new pages can be evaluated by topic evaluator, and the discounted cumulative reward (the value Q) of the links can be predicted by Q-predictor. The value Q is used to cut off the off-topic links, while the reward generated directly by the on-topic pages will be feedback along the link-chain to update all the value Q of the links. And then the characteristic text is selected as unlabeled trained data samples to incrementally improve the knowledge of the topic-evaluator and the Q-predictor. Experiment results show that this method is adaptive and rapid, and the number and accuracy of the retrieved pages can be higher than the off-line focused crawling method. So it is more suitable for discovering Web resources.
{fenge}
4043065662	Rough set classification rules mining based on incremental genetic algorithm	The rough set classification rules mining based on incremental genetic algorithm (GA) was studied from two aspects: decision rules acquisition and optimization. Knowledge theory based on rough set representation and measure was constructed according to coefficients of the decision rule and decision table. To acquire optimal classification rules, the proposed method combines GA with the rough set classification rules mining algorithm. Furthermore, the rules, in incremental form, acquired by GA were optimized. Experimental results show that it performs well in the task of optimization. Comparing with the general GA it enhances the classification precision, performs task with less run time, and more understandable result can be obtained.
{fenge}
31844438906	Two-phase genetic-annealing algorithm for vehicle routing problem with multiple constraints	A novel two-phase genetic-annealing algorithm is proposed to solve the vehicle routing problem with time window (MDVRPTW) and multi-constraint in multiple dispatching centers. In the first phase, users are partitioned into fuzzy regions according to quantity supplied and the length of paths using genetic algorithm; in the second phase the global optimization is carried out by the hybrid genetic algorithm with 2D variable-length chromosomes and corresponding genetic operators. The random greedy algorithm is used in generating of initial population and crossover and mutation operator to avoid invalid solution, then the simulated annealing algorithm is employed to enhance the diversity of population. The experimental results show that compared with the traditional genetic algorithm the search speed of the proposed algorithm is 3-10 times faster, the convergence is speed up, and search efficiency is increased.
{fenge}
31844439835	Distributed information search based on topic partition in structured peer-to-peer networks	A topic overlay network search (TONS) algorithm, P2P search mechanism based on topic partition, is presented. On the basis of the structured networks, the nodes are organized as an overlay network according to topics such that the nodes containing similar topic are linked together. Thus, the query contents can be limited in the local range of P2P network and the overlay network has small world traits by randomly adding some long distance links in the overlay network. TONS provides structured P2P networks with effective approach to search for node data objects based on complicated queries with partial match and multiple keywords. Compared with the existing structured systems, TONS increases the search recall by 74.7%, and reduces the average path distance and the average number of messages during the searching process.
{fenge}
4444239133	Fuzzy clustering of incomplete nominal and numerical data	This paper defines a new distance based on the improved Levenshtein distance with the tolerance relation for incomplete nominal data, and a new similarity strategy for incomplete numerical data. Additionally, by these two dissimilarity measures, a new distance, which measures the dissimilarity of objects with nominal and numerical attributes, is constructed. Furthermore, a new hierachical clustering model is also presented for classifying incomplete nominal and numerical data. The model needn't to be specified the number of cluster centers. Experimental results show that our method clusters incomplete nominal and numerical data with polynomial time complexity and behaves better in classification of objects than Hirano's method on the balloon data set.
{fenge}
4444241189	A noise-resistant fuzzy clustering approach with probabilistic typicalities	Probabilistic typicalities are different concepts comparing with probabilistic memberships in fuzzy clustering. They give another description of relation between data points and clustering centers. In terms of function optimization techniques, this paper presents a noise-resistant fuzzy clustering approach with probabilistic typicalities. Moreover, the usage of exponential functions greatly enlarges the relativity of points to centers based on the Euclidean distance. At last, its variety is also presented and behaves better on the Iris data than the existed methods in computation precision. Experimental comparisons on noisy data and IRIS data show that our approach is hardly affected by noise and more accurate in computing the cluster centers than existed methods.
{fenge}
4444287110	Finding the optimal gene order for genetic algorithm	This paper presents a hybrid algorithm to improve the efficiency of canonical genetic algorithm. It starts by introducing rationale and techniques of genetic algorithm and its drawback. An optimal gene order finding algorithm is then presented with its application to iterations, as well as the relative genetic operators. Finally the algorithm is applied to the Traveling Salesman Problem (TSP). After each iterating, overlap vectors from best individuals are selected as the optimal gene order and used to mark some individuals for the next iteration with carefully prepared parameters. Some data sets are chosen to investigate the performance of the hybrid algorithm and the experiment results show that it performs better than canonical genetic algorithm in some instances.
{fenge}
6344223426	SX-RSRPM: A security integrated model for Web services	The conceptions about security of web services and Degree of Safety for Web Services (WS-DoS), which is a qualitative analysis index of Web Services security and is influenced by the degree of credible environments around WS-based applications and the duration of web service execution time, are introduced in the paper. In addition, a securing logical hierarchical structure for Web Service application based on an extended Web Services security architecture model with five elemental objects, such as Resources, Services, Roles, Protocols and Methods object is analyzed, which can effectively increase the value of WS-DoS by a new affiliated role as security certification and authentication center and the extended security of protocol stacks. Moreover, an integrated security solution has been developed and the results of application show that this solution builds a confidence and authentication security environment for all roles in the process of dynamic B2B trade.
{fenge}
6344287768	Information query immune algorithm based on vector space model	To efficiently satisfy the user query requirements in information retrieval, a novel immune query optimization algorithm for information retrieval is proposed. The core of the immune algorithm lies on constructing the immune operator that is realized by vaccination and immune selection. The strategies and the methods of selecting and constructing a vaccine for the problem are given in the paper. Immune algorithm for query optimization introduces the immune operators to genetic algorithms for query optimization. This algorithm properly deals with the degeneration in conventional genetic algorithms, therefore increases the convergence speed. Experimental results show that the novel algorithm has higher precision and faster computation speed.
{fenge}
7244232911	Method of incremental construction of heterogeneous neural network ensemble with negative correlation	A new method for constructing a heterogeneous neural network ensemble (HNNE) based on heterogeneous neural network with negative correlation was presented. In training member network, the proposed method adjusts both the network architecture of the individual networks and the connection weights. So it improves the accuracy of the member neural networks while increasing the diversity among member networks and decreasing the generalization error of network ensemble. The method consists of two parts; constructing the best heterogeneous neural networks (BHNN) and constructing HNNE. The former constructs dynamically many best neural networks based on negative learning; the latter incrementally constructs neural network ensemble (NNE) by using the learned best networks. The whole ensemble process can be completed automatically with the predefined network generalization error and the NNE generalization error without priori knowledge of specifying the network structure. Experiments results with regression and classification problems show that the error rate can be improved by HNNE from 17% to 85%, better than Boosting, Bagging and other network ensemble methods.
{fenge}
10644230242	Artificial immune algorithm for flow-shop scheduling	To efficiently deal with flow-shop scheduling problems, a novel algorithm, artificial immune algorithm is proposed which is inspired by the immune system of human and other mammals to simulate the process of the interaction between antigens, antibodies and lymphocytes. The implement of the artificial immune algorithm on flow-shop problems is as follows. The objective function and the part of inequality constraints serve as antigens and solutions serve as antibodies; the antibodies are encoded as natural number that is consistent with workpiece processing sequence; the fitness function is designed as the inversion of maximal flow time. New antibodies are produced by adopting the partially matched crossover operator and mutation operator permuted by work-pieces sequence. The promotion and suppression of antibodies are adjusted according to antibody concentration that is obtained from the maximal affinity value among antibodies. The proposed algorithm is tested on scheduling problem benchmarks. Experimental results show that immune algorithm is quite flexible with satisfactory results, and requires fewer running time than genetic and simulated anneal algorithms.
{fenge}
10644263281	Knowledge reduction methods in fuzzy objective information systems	Fuzzy objective information systems (FOISs) exist in many applications, and knowledge reduction in them can't be implemented by reduction methods in Pawlak information systems. This paper firstly presents new reduction methods including α distribution, α maximum distribution, α assignment, and rough distribution reductions. It then probes into their properties and the relation between them and the reduction methods on Pawlak information systems. Furthermore, this paper proposes the judgement theorems and discernibility matrixes with respect to these reductions. These reductions extend the corresponding methods in Pawlak information systems and provide a new and low computation complexity way for knowledge discovery and rough-fuzzy rule based fuzzy concept classifiers in fuzzy objective information systems.
{fenge}
10644272708	Research and implementation of network teaching model based on role-assignment-task	In order to cope with the emergence of the multimedia and electronic features in network teaching, the assignment and task were defined and modeled from the ontology perspective in the process of network teaching. The concept of assignment state space was proposed and a network teaching management model, NERAT (network teaching role-assignment-task) model, was built based on these definitions. NERAT has a number of features with regarding students as the teaching centre and with task-driven multi-roles interaction, and provides four layers of teaching feedback strategies, such as role-based personalization feedback, task documents-based learning feedback, teaching feedback and multi-role feedback. The results of the application of OPEN-CLASS system, which is a network teaching management software platfrom developed according to NERAT, show that the model can improve the personalization learning and mutual interaction greatly in the web teaching process.
{fenge}
12844280443	Roughness of Choquet fuzzy integral and information fusion	Based on the analysis of existed fuzzy integral theories, the notions of the Choquet upper and lower fuzzy integral and rough interval numbers' integral form were proposed; the rough properties of Choquet fuzzy integral and an information fusion method based on rough Choquet fuzzy integral were given. The method can properly carry out the selection and fusion of interval number sensors in multi-sensor fusion environment in which various disturbance factors exist. The Choquet upper and lower fuzzy integral is the extended form of the classical Choquet one in a sense of roughness, which can deal with the data sources of rough intervals and objects of information systems. The simulated experiments have validated the effectiveness of the method.
{fenge}
13644284539	Web service-oriented dynamic E-business integration framework	To improve the integration capability and expansibility in web services-based Dynamic E-Business (DEB) application, four kinds of application meta-modes about web service used in DEB environment were pointed out: Consumer-Oriented Web Services for B2C E-Business (COWS), Business-Oriented Web Services for B2B E-Business (BOWS), Wireless Device-Oriented Web Services for Extensive E-Business (DOWS) and System-Oriented Web Services for Collaborative E-Business (SOWS). Furthermore, an integration web services application framework model, i.e. Web Services-oriented Dynamic E-Business Framework (WS-DEBF) model, which was based on a four-layer logic structure with application layer, resource share and information exchange layer, data resource layer and network layer, was investigated. This model provided a dynamic integration mechanism among homogeneous platforms in inner-corporation's close-coupled systems or heterogeneous platforms in inter-corporation's loose-coupled systems. In addition, an integration prototype system based on the WS-DEBF model was developed. Application results show that the WS-DEBF model can greatly improve the integration capability and expansibility and provide an effective support for complex dynamic E-business integration and implementation.
